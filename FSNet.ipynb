{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSMod(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, conv_stride=1, conv_pad=0, conv_groups=1, conv_dilation=1):\n",
    "        '''\n",
    "        input ksize: (S1, S2)\n",
    "        input channel: C\n",
    "        FS stride: (x, y)\n",
    "        output channel: filter_num\n",
    "        '''\n",
    "        super(FSMod, self).__init__()\n",
    "        \n",
    "        self.C = in_channels\n",
    "        self.S1, self.S2 = kernel_size\n",
    "        \n",
    "        if self.S1 == 3 and self.S2 == 3:\n",
    "            self.x = self.y = 2\n",
    "            K3_dict = dict([[12, 2], [16, 4], [32, 2], [64, 4], [128, 4], [256, 4], [512, 8]])\n",
    "            self.K3 = K3_dict[self.C]\n",
    "            self.K1, self.K2 = self.get_div(out_channels // self.K3)\n",
    "            self.z = int(self.C / self.K3)\n",
    "        elif self.S1 == 1 and self.S2 == 1:\n",
    "            self.x = self.y = 1\n",
    "            self.K1 = self.K2 = 1\n",
    "            self.K3 = out_channels\n",
    "            self.z = int(self.C / self.K3)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "            \n",
    "        self.K = self.K1*self.K2*self.K3\n",
    "        assert out_channels == self.K, f'invalid filter num, K={self.K}, out_channels={out_channels}'\n",
    "        \n",
    "\n",
    "        self.FS = nn.Parameter(nn.init.kaiming_normal_(torch.zeros((self.C, self.K1*self.x, self.K2*self.y)))).cuda()\n",
    "        \n",
    "        self.grad_index = torch.zeros((self.FS.size()[0]*2, self.FS.size()[1]*2, self.FS.size()[2]*2)).cuda()\n",
    "        \n",
    "        self.conv_real = nn.Conv2d(in_channels=self.C, \n",
    "                                   out_channels=self.K, \n",
    "                                   kernel_size=(self.S1, self.S2), \n",
    "                                   stride=conv_stride, \n",
    "                                   padding=conv_pad,\n",
    "                                   groups=conv_groups,\n",
    "                                   dilation=conv_dilation,\n",
    "                                   bias=False).cuda()\n",
    "        \n",
    "        self.backward_hook_handle = self.conv_real.register_backward_hook(self.backward_hook)\n",
    "        \n",
    "    def get_div(self, n):\n",
    "        divisors = []\n",
    "        for i in range(1, int(n**0.5)+1):\n",
    "            if n % i == 0:\n",
    "                divisors.append((i, n//i))\n",
    "        divisors = sorted(divisors, key=lambda x: abs(x[0] - x[1]))\n",
    "        return divisors[0]\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        \"\"\"\n",
    "        input: (N, C, H, W)\n",
    "        \"\"\"\n",
    "        self.grad_index = torch.zeros((self.FS.size()[0]*2, self.FS.size()[1]*2, self.FS.size()[2]*2)).cuda()\n",
    "        conv_weight = torch.zeros((self.K1*self.K2*self.K3, self.C, self.S1, self.S2)).cuda()\n",
    "        combinations = [(i, j, k) for i in range(self.K1) \n",
    "                        for j in range(self.K2) \n",
    "                        for k in range(self.K3)]\n",
    "        FS_extend = torch.cat([self.FS, self.FS], dim=0)\n",
    "        FS_extend = torch.cat([FS_extend, FS_extend], dim=1)\n",
    "        FS_extend = torch.cat([FS_extend, FS_extend], dim=2)\n",
    "        \n",
    "        for (k1, k2, k3) in combinations:\n",
    "            conv_weight[k1 + k2 * self.K1 + k3 * self.K1 * self.K2] = FS_extend[k3*self.z:k3*self.z+self.C, \n",
    "                                                            k1*self.x:k1*self.x+self.S1,\n",
    "                                                           k2*self.y:k2*self.y+self.S2]\n",
    "            self.grad_index[k3*self.z:k3*self.z+self.C,\n",
    "                            k1*self.x:k1*self.x+self.S1,\n",
    "                            k2*self.y:k2*self.y+self.S2] += 1\n",
    "        c, h, w = self.grad_index.size()\n",
    "        self.grad_index[:c//2, :, :] += self.grad_index[c//2:, :, :]\n",
    "        self.grad_index[:, :h//2, :] += self.grad_index[:, h//2:, :]\n",
    "        self.grad_index[:, :, :w//2] += self.grad_index[:, :, w//2:]\n",
    "        self.grad_index = self.grad_index[:c//2, :h//2, :w//2]\n",
    "        \n",
    "        self.conv_real.weight = nn.Parameter(conv_weight.cuda(), requires_grad=True)\n",
    "        \n",
    "        return self.conv_real(input)\n",
    "    \n",
    "    def backward_hook(self, module, grad_input, grad_output):\n",
    "        '''\n",
    "        grad_input[1] is the grad of weight of conv_real\n",
    "        '''\n",
    "        grad_extend = torch.zeros((self.FS.size()[0]*2, self.FS.size()[1]*2, self.FS.size()[2]*2)).cuda()\n",
    "        for i, grad in enumerate(grad_input[1]):\n",
    "            k1 = i%self.K1\n",
    "            k2 = (i // self.K1) % self.K2\n",
    "            k3 = i // (self.K1 * self.K2)\n",
    "            grad_extend[k3*self.z:k3*self.z+self.C,\n",
    "                        k1*self.x:k1*self.x+self.S1,\n",
    "                        k2*self.y:k2*self.y+self.S2] = grad\n",
    "        c, h, w = grad_extend.size()\n",
    "        grad_extend[:c//2, :, :] += grad_extend[c//2:, :, :]\n",
    "        grad_extend[:, :h//2, :] += grad_extend[:, h//2:, :]\n",
    "        grad_extend[:, :, :w//2] += grad_extend[:, :, w//2:]\n",
    "        grad_extend = grad_extend[:c//2, :h//2, :w//2]\n",
    "        grad_extend = grad_extend / self.grad_index\n",
    "        self.FS.grad = grad_extend\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    return FSMod(in_planes, out_planes, kernel_size=(3,3), conv_stride=stride,\n",
    "                conv_pad=dilation, conv_groups=groups, conv_dilation=dilation)\n",
    "    #return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "    #                padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def fs1x1(in_planes, out_planes, stride=1):\n",
    "    return FSMod(in_planes, out_planes, kernel_size=(1,1), conv_stride=stride)\n",
    "    #return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.fs1 = fs3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fs2 = fs3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.fs1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fs2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.fs1 = fs1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.fs2 = fs3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.fs3 = fs1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.fs1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fs2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.fs3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FSNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(FSNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                fs1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def make_fsnet(model_num, **kwargs):\n",
    "    assert model_num in [18, 34, 50, 101, 152], \"invalid model_num\"\n",
    "    if model_num == 18:\n",
    "        model = FSNet(BasicBlock, [2,2,2,2], **kwargs)\n",
    "    if model_num == 34:\n",
    "        model = FSNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if model_num == 50:\n",
    "        model = FSNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if model_num == 101:\n",
    "        model = FSNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if model_num == 152:\n",
    "        model = FSNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsnet18 = make_fsnet(18).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/data/unagi0/kamata', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_bsize = 128\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "test_bsize = 100\n",
    "testset = torchvision.datasets.CIFAR10(root='/data/unagi0/kamata', train=False,download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(fsnet18.parameters(), lr=0.1, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "max_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-95-e34e651aebac>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-95-e34e651aebac>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    if >= test_max_i - 1:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    '''train'''\n",
    "    fsnet18 = fsnet18.train()\n",
    "    train_loss_per_epoch = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        if i >= train_max_i - 1:\n",
    "            break\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        inputs.requires_grad_()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = fsnet18(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        train_loss_per_epoch += loss.cpu().numpy() \n",
    "        optimizer.step()\n",
    "    train_loss_per_epoch /= len(trainloader)\n",
    "    \n",
    "    '''test'''\n",
    "    fsnet18 = fsnet18.eval()\n",
    "    test_loss_per_epoch = 0\n",
    "    for i, data in enumerate(testloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        outputs = fsnet18(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss_per_epoch += loss.cpu().numpy()\n",
    "    test_loss_per_epoch /= len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
